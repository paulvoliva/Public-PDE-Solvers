{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pde-solver NCDE GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOngTDCmUjqTmmxM5h2F/kN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulvoliva/Public-PDE-Solvers/blob/main/pde_solver_NCDE_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzbGF9b_NMaL",
        "outputId": "6ed2bce6-a95c-4617-9c75-a523f46d2b48"
      },
      "source": [
        "pip install torchdiffeq"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading https://files.pythonhosted.org/packages/90/e4/5e483dc28a0a520e403f4dade7ad120d739471693afe83eaf36c9cc09cb0/torchdiffeq-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from torchdiffeq) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->torchdiffeq) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->torchdiffeq) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->torchdiffeq) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->torchdiffeq) (1.19.5)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsBiDgaMNkpa",
        "outputId": "0295ccba-e80e-4947-9bce-e92ea7bfadaf"
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 23.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 22.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (51.3.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnqQhmGuMNIu",
        "outputId": "d74ef7dd-ab2d-468c-8dfb-1c312376e200"
      },
      "source": [
        "pip install signatory"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting signatory\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/93/cea761d492f908133d513fb1f93eb57bd0839b8ecd9eda0e97848a1c3c8f/signatory-1.2.4.1.7.1.tar.gz (61kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20kB 20.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 40kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 51kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: signatory\n",
            "  Building wheel for signatory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for signatory: filename=signatory-1.2.4.1.7.1-cp36-cp36m-linux_x86_64.whl size=6769063 sha256=14f29af3a0dad723aa6c9583031401119c4950009c676c0d7e84c2bd45e04205\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/6e/dc/5762aa0bebc28a8a6bba71e3eff188534a95dadd45402a3328\n",
            "Successfully built signatory\n",
            "Installing collected packages: signatory\n",
            "Successfully installed signatory-1.2.4.1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQvEXclFM3yG",
        "outputId": "1ed895fa-c681-4efc-80fa-a6a217db6b4b"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBqqFXdJM68p",
        "outputId": "810634c1-1473-4d05-c15f-85bc8a3b7651"
      },
      "source": [
        "pip install ray"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ray\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/e5/0ff593f053ff4fa2a582961272ef893c21d268d3c2c52ff1e7effd891e48/ray-1.1.0-cp36-cp36m-manylinux2014_x86_64.whl (48.5MB)\n",
            "\u001b[K     |████████████████████████████████| 48.5MB 98kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray) (3.12.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray) (3.13)\n",
            "Collecting opencensus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/d6/b952f11b29c3a0cbec5620de3c4260cecd8c4329d83e91587edb48691e15/opencensus-0.7.12-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 63.9MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray) (1.19.5)\n",
            "Collecting aioredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.3MB/s \n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/b7/2056a6f06adb93f679f2a1e415dd33219b7c66ba69b8fd2ff1668b8064ed/py_spy-0.3.4-py2.py3-none-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray) (0.9.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray) (7.1.2)\n",
            "Collecting colorful\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 59.6MB/s \n",
            "\u001b[?25hCollecting redis>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray) (2.6.0)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray) (51.3.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray) (1.16.0)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.0.4)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 67.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (20.3.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.7.4.3)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 61.1MB/s \n",
            "\u001b[?25hCollecting hiredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/7d/6acf1c8d4f2fb327ff6feec000b4c56a20628fbe966a4c7cd16c0b80343c/hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (2020.12.5)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.17.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.52.0)\n",
            "Collecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.6)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n",
            "Building wheels for collected packages: gpustat, idna-ssl, contextvars\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp36-none-any.whl size=12622 sha256=90f99c089d5c766fa6475280a1e75d7369c813c61f549be0d60e1688a72b2dbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=2712b2c4dbefeb766706dd6e9973660b1040979ccd89cd4c4881255afde426b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7667 sha256=bdd9c9b9d1c21ad522f1fdedd8e1d13e56e61d0308f4ffd8434b108dd61956a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built gpustat idna-ssl contextvars\n",
            "Installing collected packages: immutables, contextvars, opencensus-context, opencensus, multidict, async-timeout, idna-ssl, yarl, aiohttp, hiredis, aioredis, colorama, blessings, gpustat, py-spy, colorful, redis, aiohttp-cors, ray\n",
            "Successfully installed aiohttp-3.7.3 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 contextvars-2.4 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 immutables-0.14 multidict-5.1.0 opencensus-0.7.12 opencensus-context-0.1.2 py-spy-0.3.4 ray-1.1.0 redis-3.5.3 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "cwoXHYqwuN-q",
        "outputId": "b96db76c-2e0e-4771-9b77-20e0dea8c50c"
      },
      "source": [
        "from dataset import Path, FixedCDEDataset, FlexibleCDEDataset, SubsampleDataset\n",
        "from scalers import TrickScaler\n",
        "from intervals import FixedIntervalSampler, RandomSampler, BatchIntervalSampler, create_interval_dataloader\n",
        "from functions import torch_ffill\n",
        "from rdeint import rdeint\n",
        "from model import NeuralRDE\n",
        "import torch\n",
        "import math\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import os\n",
        "import signatory\n",
        "\n",
        "'''Given a data tensor of shape [N, L, C] that is filled with nan values, and a corresponding times tensor of shape\n",
        "    [N, L] the corresponds to the time the data was collected for each row\n",
        "    \n",
        "    The data of shape [N, L, C]. It is assumed that the times are in the first index of the\n",
        "    data channels.\n",
        "    \n",
        "    N = number of paths\n",
        "    L = time dim\n",
        "    C = channels'''\n",
        "\n",
        "'''\n",
        "# I use this to get the shape of the tensor when I debug\n",
        "old_repr = torch.Tensor.__repr__\n",
        "def tensor_info(tensor):\n",
        "    return repr(tensor.shape)[6:] + ' ' + repr(tensor.dtype)[6:] + '@' + str(tensor.device) + '\\n' + old_repr(tensor)\n",
        "torch.Tensor.__repr__ = tensor_info\n",
        "#'''\n",
        "\n",
        "def train(config, checkpoint_dir=None):\n",
        "\n",
        "    # setting to cuda\n",
        "    if torch.cuda.is_available():\n",
        "      dev = \"cuda:0\"\n",
        "    else:\n",
        "      dev = \"cpu\"\n",
        "\n",
        "    device = torch.device(dev)\n",
        "    print(device)\n",
        "\n",
        "    ''' # Parameters'''\n",
        "\n",
        "    npaths = 20\n",
        "    intervals = 20\n",
        "    step = 10\n",
        "    t_mesh_size = int(step*intervals)\n",
        "    batch_size = npaths\n",
        "    depth = 1\n",
        "\n",
        "    assert npaths==batch_size, \"Warning! The code does not yet support actual batches of data, modify the training function to do so\"\n",
        "\n",
        "    bnpaths = 20 #number of paths on the boundary (will be multiplied by 4)\n",
        "\n",
        "    up = 1\n",
        "    down = -1\n",
        "    T = 100             #remember to divide this by 100 in all of the functions\n",
        "    T0 = 0\n",
        "\n",
        "    ''' # Dataset '''\n",
        "\n",
        "    x = torch.Tensor(npaths, t_mesh_size, 1).uniform_(down, up).requires_grad_(True).to(device)\n",
        "    y = torch.Tensor(npaths, t_mesh_size, 1).uniform_(down, up).requires_grad_(True).to(device)\n",
        "    t = torch.linspace(T0, T, t_mesh_size).unsqueeze(1).unsqueeze(2).view(1, t_mesh_size, 1).repeat(npaths, 1, 1).requires_grad_(True).to(device)\n",
        "\n",
        "    xv = x[:, ::step, :].clone().detach().requires_grad_(True).to(device)\n",
        "    yv = y[:, ::step, :].clone().detach().requires_grad_(True).to(device)\n",
        "    tv = t[:, ::step, :].clone().detach().requires_grad_(True).to(device)\n",
        "\n",
        "    txy = torch.cat((t, x, y), dim=2).to(device)\n",
        "\n",
        "    dataset = FlexibleCDEDataset(txy, torch.ones_like(txy)[:, :, 0], depth=depth)\n",
        "    sampler = FixedIntervalSampler(t_mesh_size, step, from_start=True, include_end=False)\n",
        "\n",
        "    dataloader = create_interval_dataloader(dataset, sampler, batch_size)\n",
        "\n",
        "    # boundary points\n",
        "\n",
        "    r1, r2, r3, r4 = torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(down, up), torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(down, up), torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(down, up), torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(down, up)\n",
        "    ones = torch.ones(bnpaths, t_mesh_size, 1)\n",
        "    t1, t2, t3, t4 = torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(T0, T), torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(T0, T), torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(T0, T), torch.Tensor(bnpaths, t_mesh_size, 1).uniform_(T0, T)\n",
        "\n",
        "    bl, br, bt, bb = torch.cat((t1, -ones, r1), dim=2), torch.cat((t2, ones, r2), dim=2), torch.cat((t3, r3, ones), dim=2), torch.cat((t4, r4, -ones), dim=2)\n",
        "    btxy = torch.cat((bl, br, bb, bt), dim=0)\n",
        "\n",
        "    borderset = FlexibleCDEDataset(btxy, torch.ones_like(btxy)[:, :, 0], depth=depth)\n",
        "    borderloader = create_interval_dataloader(borderset, sampler, 4*bnpaths)\n",
        "\n",
        "    ''' # PDE functions '''\n",
        "\n",
        "    def func_u_sol(x, y, t):\n",
        "        u = 2 * torch.sin(math.pi / 2 * x) * torch.cos(math.pi / 2 * y) * torch.exp(t/100)\n",
        "        return(u)\n",
        "\n",
        "    def func_f(x, y, t):\n",
        "        f = (math.pi ** 2 - 2) * torch.sin(math.pi / 2 * x) * torch.cos(math.pi / 2 * y) * torch.exp(\n",
        "            -t/100) - 4 * torch.sin(math.pi / 2 * x) ** 2 * torch.cos(math.pi / 2 * y) * torch.exp(-t/100)\n",
        "        return(f)\n",
        "\n",
        "    def func_g(bx, by, bt):\n",
        "        # bx, by, bt denote the boundary coordinates\n",
        "        return func_u_sol(bx, by, bt)\n",
        "\n",
        "    def func_h(x, y):\n",
        "        h = 2 * torch.sin(math.pi / 2 * x) * torch.cos(math.pi / 2 * y)\n",
        "        return h\n",
        "\n",
        "    def func_w(x):  # returns 1 for positions in the domain and 0 otherwise\n",
        "        w_bool = torch.gt(1 - torch.abs(x), torch.zeros(x.shape).to(device)) & torch.gt(torch.abs(x), torch.zeros(x.shape).to(device))\n",
        "        w_val = torch.where(w_bool, 1 - torch.abs(x) + torch.abs(x), torch.zeros(x.shape).to(device))\n",
        "        return w_val\n",
        "\n",
        "    ''' # Model'''\n",
        "\n",
        "    # The generator model is just the NeuralRDE model\n",
        "\n",
        "    class discriminator(torch.nn.Module):  # this makes the v function\n",
        "        def __init__(self, config):\n",
        "            super().__init__()\n",
        "            self.num_layers = config['v_layers']\n",
        "            self.hidden_dim = config['v_hidden_dim']\n",
        "            self.input = torch.nn.Linear(3, self.hidden_dim)\n",
        "            self.hidden = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "            self.output = torch.nn.Linear(self.hidden_dim, 1)\n",
        "            self.net = torch.nn.Sequential(*[\n",
        "                self.input,\n",
        "                *[torch.nn.ReLU(), self.hidden] * self.num_layers,\n",
        "                torch.nn.Tanh(),\n",
        "                self.output\n",
        "\n",
        "            ])\n",
        "\n",
        "        def forward(self, x, y, t):\n",
        "            inp = torch.cat((t, x, y), dim=2)\n",
        "            x = self.net(inp)\n",
        "            return x\n",
        "\n",
        "        def backward(self, retain_graph=True):\n",
        "            self.loss.backward(retain_graph=retain_graph)\n",
        "            return (self.loss)\n",
        "\n",
        "    ''' # Loss '''\n",
        "\n",
        "    def I(y_output_u, y_output_v, xv, yv, tv, x, y, t):\n",
        "        y_output_u.retain_grad()\n",
        "        y_output_v.retain_grad()\n",
        "        phi = y_output_v * func_w(xv)\n",
        "        y_output_u.backward(torch.ones_like(y_output_u), retain_graph=True)\n",
        "        du_x = x.grad[:, ::step, :]\n",
        "        du_y = y.grad[:, ::step, :]\n",
        "        phi.backward(torch.ones_like(phi), retain_graph=True)\n",
        "        dphi_x = xv.grad\n",
        "        dphi_y = yv.grad\n",
        "        dphi_t = tv.grad\n",
        "        s1 = y_output_u[:, -1, :] * phi[:, -1, :] - func_h(x[:, 0, :], y[:, 0, :]) * phi[:, 0, :]\n",
        "        s2 = (y_output_u * dphi_t)/t_mesh_size*step  # for t does this make sense?\n",
        "        Lap = du_x * dphi_x + du_y * dphi_y\n",
        "        s3 = (T-T0)*(Lap + y_output_u * y_output_u * phi - func_f(x[:, ::step, :], y[:, ::step, :], t[:, ::step, :]) * phi)/t_mesh_size*step\n",
        "        I = torch.sum(s1 - torch.sum(s2 - s3, 1), 0)\n",
        "        x.grad.data.zero_()\n",
        "        y.grad.data.zero_()\n",
        "        xv.grad.data.zero_()\n",
        "        yv.grad.data.zero_()\n",
        "        tv.grad.data.zero_()\n",
        "        return I\n",
        "\n",
        "\n",
        "    def L_init(y_output_u):\n",
        "        return torch.mean((y_output_u[:, 0, :] - func_h(x[:, 0, :], y[:, 0, :]) ** 2))\n",
        "\n",
        "\n",
        "    def L_bdry(u_net, border_logsig):\n",
        "        return torch.mean((u_net(border_logsig) - func_g(btxy[:, ::step, 1], btxy[:, ::step, 2], btxy[:, ::step, 0]).unsqueeze(2)) ** 2)\n",
        "\n",
        "\n",
        "    def L_int(y_output_u, y_output_v, xv=xv, yv=yv, tv=tv, x=x, y=y, t=t):\n",
        "        # x needs to be the set of points set plugged into net_u and net_v\n",
        "        return torch.log((I(y_output_u, y_output_v, xv, yv, tv, x, y, t)) ** 2) - torch.log(torch.sum(y_output_v ** 2))\n",
        "\n",
        "\n",
        "    def Loss_u(y_output_u, y_output_v, border_logsig, u_net, alpha, gamma):\n",
        "        return L_int(y_output_u, y_output_v) + gamma * L_init(y_output_u) + alpha * L_bdry(\n",
        "            u_net, border_logsig)\n",
        "\n",
        "    def Loss_v(y_output_u, y_output_v):\n",
        "        return -L_int(y_output_u, y_output_v)\n",
        "\n",
        "    iteration = 2000\n",
        "\n",
        "    (initial, logsig), responses = next(iter(dataloader))\n",
        "    logsig_dim = logsig.shape[2]\n",
        "\n",
        "    '''\n",
        "    batch, reponses = next(iter(dataloader))\n",
        "    batchborder, responses = next(iter(borderloader))\n",
        "    '''\n",
        "\n",
        "\n",
        "    n1 = config['n1']\n",
        "    n2 = config['n2']\n",
        "\n",
        "    # neural network models\n",
        "    u_net = NeuralRDE(3, logsig_dim, config['u_hidden_dim'], 1, hidden_hidden_dim=config['u_hidden_hidden_dim'], num_layers=config['u_layers'], return_sequences=True).to(device)\n",
        "    v_net = discriminator(config).to(device)\n",
        "\n",
        "    # optimizers for WAN\n",
        "    optimizer_u = torch.optim.Adam(u_net.parameters(), lr=config['u_rate'])\n",
        "    optimizer_v = torch.optim.Adam(v_net.parameters(), lr=config['v_rate'])\n",
        "\n",
        "    for batch, responses in dataloader:\n",
        "        prediction_u = u_net(batch)\n",
        "    prediction_v = v_net(xv, yv, tv)\n",
        "\n",
        "    Loss = 0\n",
        "\n",
        "    for k in range(iteration):\n",
        "\n",
        "        for i in range(n1):\n",
        "            for batch, responses in dataloader:\n",
        "                for batchborder, responses in borderloader:\n",
        "                    (initial, logsig), (binitial, blogsig) = batch, batchborder\n",
        "                    batch, batchborder = (initial.to(device), logsig.to(device)), (binitial.to(device), blogsig.to(device))\n",
        "                    loss_u = Loss_u(prediction_u, prediction_v, batchborder, u_net, config['alpha'], config['alpha'])\n",
        "                    optimizer_u.zero_grad()\n",
        "                    loss_u.backward(retain_graph=True)\n",
        "                    optimizer_u.step()\n",
        "                    prediction_u = u_net(batch)\n",
        "\n",
        "        for j in range(n2):\n",
        "            loss_v = Loss_v(prediction_u, prediction_v)\n",
        "            optimizer_v.zero_grad()\n",
        "            loss_v.backward(retain_graph=True)\n",
        "            optimizer_v.step()\n",
        "            prediction_v = v_net(xv, yv, tv)\n",
        "\n",
        "        Loss += 0.1*loss_u\n",
        "\n",
        "        if k % 10 == 0:\n",
        "            print(k, loss_u.data, loss_v.data)\n",
        "            error_test = torch.mean(\n",
        "                torch.sqrt(torch.square((func_u_sol(x[:, ::step, :], y[:, ::step, :], t[:, ::step, :]) - prediction_u.data)))).data\n",
        "            print(\"error test \" + str(error_test))\n",
        "            tune.report(Loss=Loss)\n",
        "\n",
        "            with tune.checkpoint_dir(k) as checkpoint_dir:\n",
        "                path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "                torch.save((u_net.state_dict(), v_net.state_dict()), path)\n",
        "'''\n",
        "config = {\n",
        "    'v_layers': 5,\n",
        "    'v_hidden_dim': 10,\n",
        "    'u_hidden_dim': 15,\n",
        "    'u_hidden_hidden_dim': 5,\n",
        "    'u_layers': 5,\n",
        "    'alpha': 25,  # 1e5*boundary_sample_size*4 # 25\n",
        "    'n1': 2,\n",
        "    'n2': 1,\n",
        "    'u_rate': 0.0015,       # 0.0015\n",
        "    'v_rate': 0.0015       # 0.0015\n",
        "}\n",
        "'''\n",
        "config = {\n",
        "    'v_layers': tune.qrandint(2, 10),   # 5\n",
        "    'v_hidden_dim': tune.qrandint(5, 20),  # 10\n",
        "    'u_hidden_dim': tune.qrandint(5, 20),\n",
        "    'u_hidden_hidden_dim': tune.qrandint(5, 20),\n",
        "    'u_layers': tune.qrandint(5, 20),\n",
        "    'alpha': tune.loguniform(10, 1e7),  # 1e5*boundary_sample_size*4 # 25\n",
        "    'n1': 2,\n",
        "    'n2': 1,\n",
        "    'u_rate': tune.loguniform(1e-6, 1e-2),       # 0.0015\n",
        "    'v_rate': tune.loguniform(1e-6, 1e-2)       # 0.0015\n",
        "}\n",
        "\n",
        "\n",
        "#train(config)\n",
        "\n",
        "tune.utils.diagnose_serialization(train)\n",
        "\n",
        "analysis = tune.run(\n",
        "    train,\n",
        "    num_samples=200,\n",
        "    scheduler=ASHAScheduler(metric=\"Loss\", mode=\"min\", grace_period=10, max_t=200, reduction_factor=4),\n",
        "    config=config,\n",
        "    verbose=2,\n",
        "    resources_per_trial={'cpu':1}\n",
        ")\n",
        "\n",
        "best_trial = analysis.get_best_trial(metric=\"Loss\", mode=\"min\")\n",
        "print(\"Best trial config: {}\".format(best_trial.config))\n",
        "\n",
        "'''\n",
        "best_trained_generator = NeuralRDE(3, logsig_dim, best_trial.config['u_hidden_dim'], 1, hidden_hidden_dim=best_trial.config['u_hidden_hidden_dim'], num_layers=best_trial.config['u_layers'], return_sequences=True)\n",
        "best_trained_discriminator = discriminator(best_trial.config)\n",
        "\n",
        "best_checkpoint_dir = best_trial.checkpoint.value\n",
        "generator_state, discriminator_state = torch.load(os.path.join(\n",
        "    best_checkpoint_dir, \"checkpoint\"))\n",
        "'''\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trying to serialize <function train at 0x7f097e099a60>...\n",
            "Serialization succeeded!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-01-22 17:48:18,385\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
            "2021-01-22 17:48:21,883\tINFO logger.py:627 -- pip install 'ray[tune]' to see TensorBoard files.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b09e04e73cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;31m# Create syncer callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     callbacks = create_default_callbacks(\n\u001b[0;32m--> 362\u001b[0;31m         callbacks, sync_config, metric=metric, loggers=loggers)\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     runner = TrialRunner(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/utils/callback.py\u001b[0m in \u001b[0;36mcreate_default_callbacks\u001b[0;34m(callbacks, sync_config, loggers, metric)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mlast_logger_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_tbx_logger\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTBXLoggerCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mlast_logger_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/logger.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}